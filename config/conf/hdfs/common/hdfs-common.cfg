[configuration]
  # The raw files need to read when generating the configuration
  configuration.xsl=${config_dir}/template/configuration.xsl
  log4j.xml=${config_dir}/template/hdfs/log4j.xml
  krb5.conf=${config_dir}/krb5-hadoop.conf
  rackinfo.txt=${config_dir}/rackinfo.txt
  hadoop-groups.conf=${config_dir}/hadoop-groups.conf
  excludes=${config_dir}/excludes
  pre.sh=${config_dir}/template/hdfs/pre.sh
  post.sh=${config_dir}/template/hdfs/post.sh

  [[core-site.xml]]
  # The core-site.xml config content
  fs.defaultFS=hdfs://${cluster.name}
  ha.zookeeper.quorum=${zk.hosts_with_port}

  hadoop.http.staticuser.user=hdfs
  hadoop.tmp.dir=/tmp/hadoop
  io.file.buffer.size=131072

  # security authentication switch
  hadoop.security.authentication=simple
  hadoop.security.authorization=false
  hadoop.security.use-weak-http-crypto=false

  # Configure to enable hdfs users can access hdfs through the webhdfs proxy user 'hue'
  hadoop.proxyuser.hue.hosts=*
  hadoop.proxyuser.hue.groups=*
  hue.kerberos.principal.shortname=hue

  [[hdfs-site.xml]]
  # The hdfs-site.xml config content
  dfs.nameservices=${cluster.name}
  dfs.ha.namenodes.${cluster.name}="task0,task1"
  dfs.client.failover.proxy.provider.${cluster.name}=org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider
  dfs.namenode.rpc-address.${cluster.name}.task0=${namenode.0.host}:${namenode.0.base_port}
  dfs.namenode.rpc-address.${cluster.name}.task1=${namenode.1.host}:${namenode.1.base_port}
  dfs.namenode.http-address.${cluster.name}.task0=${namenode.0.host}:${namenode.0.base_port+1}
  dfs.namenode.http-address.${cluster.name}.task1=${namenode.1.host}:${namenode.1.base_port+1}

  dfs.journalnode.rpc-address=0.0.0.0:${journalnode.base_port}
  dfs.journalnode.http-address=0.0.0.0:${journalnode.base_port+1}
  dfs.journalnode.edits.dir=${journalnode.data_dir}

  dfs.namenode.shared.edits.dir=qjournal://${journalnode_task_list}/${cluster.name}
  dfs.ha.zkfc.port=${zkfc.base_port}
  dfs.namenode.name.dir=${namenode.data_dir}
  net.topology.table.file.name=${namenode.run_dir}/rackinfo.txt
  net.topology.node.switch.mapping.impl=org.apache.hadoop.net.TableMapping
  dfs.hosts.exclude=${namenode.run_dir}/excludes
  hadoop.security.group.mapping.file.name=${namenode.run_dir}/hadoop-groups.conf

  dfs.block.local-path-access.user="work, hbase, hbase_srv, impala"
  dfs.datanode.ipc.address=0.0.0.0:${datanode.base_port}
  dfs.datanode.http.address=0.0.0.0:${datanode.base_port+1}
  dfs.datanode.address=0.0.0.0:${datanode.base_port+2}
  dfs.datanode.data.dir=${datanode.data_dirs}

  dfs.client.read.shortcircuit.skip.auth=true

  dfs.datanode.balance.bandwidthPerSec=10485760
  dfs.namenode.replication.min=1
  dfs.namenode.safemode.threshold-pct=0.99f

  dfs.ha.fencing.methods="sshfence&#xA;shell(/bin/true)"
  dfs.ha.fencing.ssh.private-key-files=/home/work/.ssh/id_rsa
  dfs.ha.fencing.ssh.connect-timeout=2000
  dfs.ha.automatic-failover.enabled=true

  dfs.datanode.max.xcievers=4096
  dfs.namenode.handler.count=64
  dfs.block.size=128m
  dfs.client.read.shortcircuit=true
  fs.trash.interval=10080
  fs.trash.checkpoint.interval=1440

  # config for dfs acl
  dfs.permissions=true
  dfs.web.ugi="hdfs,supergroup"
  dfs.permissions.superusergroup=supergroup
  dfs.permissions.superuser=hdfs_admin
  dfs.namenode.upgrade.permission=0777
  fs.permissions.umask-mode=022
  dfs.cluster.administrators=hdfs_admin
  # hadoop.security.group.mapping=org.apache.hadoop.security.ConfigurationBasedGroupsMapping

  # config security
  dfs.block.access.token.enable=true
  ignore.secure.ports.for.testing=true
  # namenode security config
  dfs.namenode.kerberos.internal.spnego.principal=HTTP/hadoop@EXAMPLE.HADOOP
  dfs.namenode.keytab.file=/etc/hadoop/conf/hdfs.keytab
  dfs.namenode.kerberos.principal=hdfs/hadoop@EXAMPLE.HADOOP
  # secondary namenode security config
  dfs.secondary.namenode.kerberos.internal.spnego.principal=HTTP/hadoop@EXAMPLE.HADOOP
  dfs.secondary.namenode.keytab.file=/etc/hadoop/conf/hdfs.keytab
  dfs.secondary.namenode.kerberos.principal=hdfs/hadoop@EXAMPLE.HADOOP
  # datanode security config
  dfs.datanode.data.dir.perm=700
  dfs.datanode.keytab.file=/etc/hadoop/conf/hdfs.keytab
  dfs.datanode.kerberos.principal=hdfs/hadoop@EXAMPLE.HADOOP
  # journalnode security config
  dfs.journalnode.kerberos.internal.spnego.principal=HTTP/hadoop@EXAMPLE.HADOOP
  dfs.journalnode.keytab.file=/etc/hadoop/conf/hdfs.keytab
  dfs.journalnode.kerberos.principal=hdfs/hadoop@EXAMPLE.HADOOP
  # config web
  dfs.web.authentication.kerberos.principal=HTTP/hadoop@EXAMPLE.HADOOP
  dfs.web.authentication.kerberos.keytab=/etc/hadoop/conf/hdfs.keytab

