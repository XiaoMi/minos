# configuration for kafka.cfg
[configuration]
  # The raw files need to read when generating the configuration
  log4j.properties=%{config_dir}/template/kafka/log4j.properties

  [[kafka.cfg]]
    # The kafka.cfg config content

    # Core configuration
    broker.id="task.id"  # replaced with the real task id in the deploy script.
    port=%{kafka.base_port}
    log.dir=%{kafka.data_dir}
    zookeeper.connect=%{zk.hosts_with_port}/kafka/%{cluster.name}

    # Replication configuration
    default.replication.factor=2
    num.replica.fetchers=4
    replica.fetch.max.bytes=1048576
    replica.fetch.wait.max.ms=500
    replica.high.watermark.checkpoint.interval.ms=5000
    replica.socket.timeout.ms=30000
    replica.socket.receive.buffer.bytes=65536
    replica.lag.time.max.ms=10000
    replica.lag.max.messages=4000

    controller.socket.timeout.ms=30000
    controller.message.queue.size=10

    # Log configuration
    num.partitions=8
    message.max.bytes=1000000
    auto.create.topics.enable=true
    log.index.interval.bytes=4096
    log.index.size.max.bytes=10485760
    log.retention.hours=48
    log.flush.interval.ms=10000
    log.flush.interval.messages=20000
    log.flush.scheduler.interval.ms=2000
    log.roll.hours=48
    log.cleanup.interval.mins=30
    log.segment.bytes=1073741824

    # ZK configuration
    zookeeper.connection.timeout.ms=6000
    zookeeper.sync.time.ms=2000

    # Socket server configuration
    num.io.threads=8
    num.network.threads=8
    socket.request.max.bytes=104857600
    socket.receive.buffer.bytes=1048576
    socket.send.buffer.bytes=1048576
    queued.max.requests=16
    fetch.purgatory.purge.interval.requests=100
    producer.purgatory.purge.interval.requests=100

  [[kafka-scribe.cfg]]
    # The kafka-scribe.cfg config content

    # Core configuration
    scribe.port=%{kafkascribe.base_port}
    metadata.broker.list="broker.list"  # replaced with the real broker list in the deploy script.
    request.required.acks=1
    producer.type=sync
    serializer.class=kafka.serializer.StringEncoder


[arguments]

  [[service_common]]
    jvm_args='''
      -Xmx4096m
      -Xms3072m
      -Xmn2048m
      -XX:MaxDirectMemorySize=1024m
      -XX:MaxPermSize=512m
      -XX:+DisableExplicitGC
      -XX:+HeapDumpOnOutOfMemoryError
      -XX:HeapDumpPath=$log_dir
      -XX:+PrintGCApplicationStoppedTime
      -XX:+UseConcMarkSweepGC
      -XX:CMSInitiatingOccupancyFraction=80
      -XX:+UseMembar
      -verbose:gc
      -XX:+PrintGCDetails
      -XX:+PrintGCDateStamps
      -Xloggc:$run_dir/stdout/kafka_gc_${start_time}.log
    '''

    system_properties='''
      -Djava.net.preferIPv4Stack=true
      -Dkafka.log.dir=$log_dir
      -Dkafka.log.level=%{cluster.log_level}
    '''

    main_entry='''
    '''

    extra_args='''
    '''

  [[kafka]]
    main_entry='''
      kafka.Kafka
    '''

    extra_args='''
      $run_dir/kafka.cfg
    '''

  [[kafkascribe]]
    main_entry='''
      com.xiaomi.infra.kafka.scribe.Server
    '''

    extra_args='''
      $run_dir/kafka-scribe.cfg
    '''
